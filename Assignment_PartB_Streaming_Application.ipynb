{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pprint import pprint\n",
    "\n",
    "import pygeohash as pgh\n",
    "\n",
    "os.environ[\n",
    "    \"PYSPARK_SUBMIT_ARGS\"\n",
    "] = \"--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting constant environment variables\n",
    "TOPIC = \"climate,hotspot\"  # read from 2 topics\n",
    "HOST = \"localhost\"\n",
    "BATCH_INTERVAL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/homebrew/Cellar/apache-spark/3.4.0/libexec/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/weichun/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/weichun/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a8dc0fa1-6f55-4875-a7c3-44f21167837f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 1650ms :: artifacts dl 24ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a8dc0fa1-6f55-4875-a7c3-44f21167837f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/34ms)\n",
      "23/06/05 17:31:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# initialize spark client to read from kafka source\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"Climate-Hotspot-Analysis\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "kafka_sdf = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{HOST}:9092\")\n",
    "    .option(\"subscribe\", TOPIC)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_datapoint(measurement):\n",
    "    # initialize client and connection to db\n",
    "    client = MongoClient()\n",
    "    db = client.fit3182_assignment_db\n",
    "    collection = db.climate_historic\n",
    "\n",
    "    # insert document into db per batch\n",
    "    try:\n",
    "        collection.insert_one(measurement)\n",
    "    except Exception as ex:\n",
    "        print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    client.close()\n",
    "\n",
    "\n",
    "def group_hotspots(aqua, terra):\n",
    "    groups = {}\n",
    "    fire_events = set()\n",
    "\n",
    "    # iterate through each record and group similar geohash record together\n",
    "    for record in aqua:\n",
    "        geohash = record[\"geohash\"]\n",
    "        if geohash not in groups:\n",
    "            groups[geohash] = []\n",
    "        groups[geohash].append(record)\n",
    "\n",
    "    hotspot_key = groups.keys()\n",
    "\n",
    "    # iterate through each record and group similar geohash record together\n",
    "    for record in terra:\n",
    "        geohash = record[\"geohash\"]\n",
    "        # categorize as fire_event if there is another datapoint coming from another/previous satelite\n",
    "        if geohash in hotspot_key:\n",
    "            fire_events.add(geohash)\n",
    "        if geohash not in groups:\n",
    "            groups[geohash] = []\n",
    "        groups[geohash].append(record)\n",
    "\n",
    "    hotspots = []\n",
    "    for group in groups.values():\n",
    "        if len(group) > 1:\n",
    "            # find average of measurement for hotspots data that are referring to the same location\n",
    "            number_of_records = len(group)\n",
    "            datapoint = group[0]\n",
    "\n",
    "            total_temp = 0\n",
    "            total_conf = 0\n",
    "\n",
    "            for record in group:\n",
    "                total_temp += record[\"surface_temperature_celcius\"]\n",
    "                total_conf += record[\"confidence\"]\n",
    "\n",
    "            # calculate average surface temperature and confidence in 2 decimal point\n",
    "            datapoint[\"surface_temperature_celcius\"] = round(\n",
    "                total_temp / number_of_records, 2\n",
    "            )\n",
    "            datapoint[\"confidence\"] = round(total_conf / number_of_records, 2)\n",
    "\n",
    "            hotspots.append(datapoint)\n",
    "        else:\n",
    "            hotspots.append(group[0])\n",
    "\n",
    "    return fire_events, hotspots\n",
    "\n",
    "\n",
    "def find_correlation(climate, hotspots, fire_events):\n",
    "    climate[\"hotspots\"] = []\n",
    "    if climate and hotspots:\n",
    "        for hotspot in hotspots:\n",
    "            # relate climate and hotspots data only if they are close ie\n",
    "            # same geohash with precision 3\n",
    "            if climate[\"geohash\"][:3] == hotspot[\"geohash\"][:3]:\n",
    "                # hotspots are categorized only if there are fire_events\n",
    "                if hotspot[\"geohash\"] in fire_events:\n",
    "                    if (\n",
    "                        climate[\"air_temperature_celcius\"] > 20\n",
    "                        and climate[\"GHI_w/m2\"] > 180\n",
    "                    ):\n",
    "                        hotspot[\"cause\"] = \"natural\"\n",
    "                    else:\n",
    "                        hotspot[\"cause\"] = \"other\"\n",
    "                    print(f\"{hotspot['cause']} fire detected!\")\n",
    "\n",
    "            # clean up hotspot data\n",
    "            hotspot[\"timestamp\"] = datetime.strptime(\n",
    "                hotspot[\"datetime\"], \"%Y-%m-%dT%H:%M:%S\"\n",
    "            ).strftime(\"%H:%M:%S\")\n",
    "            hotspot.pop(\"producer_id\")\n",
    "            hotspot.pop(\"geohash\")\n",
    "            hotspot.pop(\"datetime\")\n",
    "\n",
    "            climate[\"hotspots\"].append(hotspot)\n",
    "\n",
    "    # clean up climate data\n",
    "    climate.pop(\"producer_id\")\n",
    "    climate.pop(\"latitude\")\n",
    "    climate.pop(\"longitude\")\n",
    "    climate.pop(\"geohash\")\n",
    "\n",
    "    climate[\"date\"] = datetime.strptime(climate[\"date\"], \"%Y-%m-%d\")\n",
    "    return climate\n",
    "\n",
    "\n",
    "def process_batch(df, epoch_id):\n",
    "    climate = None\n",
    "    aqua = []\n",
    "    terra = []\n",
    "\n",
    "    # iterate through batch dataset and assign to appropriate list based on producer_id\n",
    "    dataset = df.collect()\n",
    "    for record in dataset:\n",
    "        data = json.loads(record.value)\n",
    "\n",
    "        # encode geohash based on latitude and longitude\n",
    "        data[\"geohash\"] = pgh.encode(data[\"latitude\"], data[\"longitude\"], precision=5)\n",
    "\n",
    "        if data[\"producer_id\"] == \"climate_producer\":\n",
    "            climate = data\n",
    "        elif data[\"producer_id\"] == \"aqua_producer\":\n",
    "            aqua.append(data)\n",
    "        elif data[\"producer_id\"] == \"terra_producer\":\n",
    "            terra.append(data)\n",
    "        else:\n",
    "            print(\"Invalid producer_id....Skipping\")\n",
    "\n",
    "    print(\n",
    "        f\"received {0 if not climate else 1} climate records, {len(aqua)} aqua records, {len(terra)} terra records\"\n",
    "    )\n",
    "\n",
    "    # only proceed to process data for current window if there's a climate data\n",
    "    # climate date form the basis of a document in the DB hence no point in processing if there is no data\n",
    "    if climate:\n",
    "        fire_events, hotspots = group_hotspots(aqua, terra)\n",
    "        measurement = find_correlation(climate, hotspots, fire_events)\n",
    "        submit_datapoint(measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure spark writer to process stream in mini batches\n",
    "writer = (\n",
    "    kafka_sdf.writeStream.format(\"Console\")\n",
    "    .option(\"checkpointLocation\", \"climate-hotspots-checkpoint\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=f\"{BATCH_INTERVAL} seconds\")  # trigger action in batches\n",
    "    .foreachBatch(process_batch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 17:31:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/05 17:31:12 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 17:31:12 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 17:31:12 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/05 17:31:12 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/05 17:31:12 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received 1 climate records, 153 aqua records, 56 terra records\n",
      "received 1 climate records, 4 aqua records, 4 terra records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received 1 climate records, 5 aqua records, 5 terra records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 2) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received 1 climate records, 5 aqua records, 5 terra records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/weichun/Desktop/fit3182/asgn_3/env/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/weichun/Desktop/fit3182/asgn_3/env/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/weichun/.pyenv/versions/3.9.16/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by CTRL-C. Stopping query.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by CTRL-C. Stopping query.\")\n",
    "finally:\n",
    "    query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
