{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pprint import pprint\n",
    "\n",
    "import pygeohash as pgh\n",
    "\n",
    "os.environ[\n",
    "    \"PYSPARK_SUBMIT_ARGS\"\n",
    "] = \"--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting constant environment variables\n",
    "TOPIC = \"climate,hotspot\"  # read from 2 topics\n",
    "HOST = \"localhost\"\n",
    "BATCH_INTERVAL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spark client to read from kafka source\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"Climate-Hotspot-Analysis\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "kafka_sdf = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{HOST}:9092\")\n",
    "    .option(\"subscribe\", TOPIC)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_datapoint(measurement):\n",
    "    # initialize client and connection to db\n",
    "    client = MongoClient()\n",
    "    db = client.fit3182_assignment_db\n",
    "    collection = db.climate_historic\n",
    "\n",
    "    # insert document into db per batch\n",
    "    try:\n",
    "        collection.insert_one(measurement)\n",
    "    except Exception as ex:\n",
    "        print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    client.close()\n",
    "\n",
    "\n",
    "def group_hotspots(aqua, terra):\n",
    "    groups = {}\n",
    "    fire_events = set()\n",
    "\n",
    "    # iterate through each record and group similar geohash record together\n",
    "    for record in aqua:\n",
    "        geohash = record[\"geohash\"]\n",
    "        if geohash not in groups:\n",
    "            groups[geohash] = []\n",
    "        groups[geohash].append(record)\n",
    "\n",
    "    hotspot_key = groups.keys()\n",
    "\n",
    "    # iterate through each record and group similar geohash record together\n",
    "    for record in terra:\n",
    "        geohash = record[\"geohash\"]\n",
    "        # categorize as fire_event if there is another datapoint coming from another/previous satelite\n",
    "        if geohash in hotspot_key:\n",
    "            fire_events.add(geohash)\n",
    "        if geohash not in groups:\n",
    "            groups[geohash] = []\n",
    "        groups[geohash].append(record)\n",
    "\n",
    "    hotspots = []\n",
    "    for group in groups.values():\n",
    "        if len(group) > 1:\n",
    "            # find average of measurement for hotspots data that are referring to the same location\n",
    "            number_of_records = len(group)\n",
    "            datapoint = group[0]\n",
    "\n",
    "            total_temp = 0\n",
    "            total_conf = 0\n",
    "\n",
    "            for record in group:\n",
    "                total_temp += record[\"surface_temperature_celcius\"]\n",
    "                total_conf += record[\"confidence\"]\n",
    "\n",
    "            # calculate average surface temperature and confidence in 2 decimal point\n",
    "            datapoint[\"surface_temperature_celcius\"] = round(\n",
    "                total_temp / number_of_records, 2\n",
    "            )\n",
    "            datapoint[\"confidence\"] = round(total_conf / number_of_records, 2)\n",
    "\n",
    "            hotspots.append(datapoint)\n",
    "        else:\n",
    "            hotspots.append(group[0])\n",
    "\n",
    "    return fire_events, hotspots\n",
    "\n",
    "\n",
    "def find_correlation(climate, hotspots, fire_events):\n",
    "    climate[\"hotspots\"] = []\n",
    "    if climate and hotspots:\n",
    "        for hotspot in hotspots:\n",
    "            # relate climate and hotspots data only if they are close ie\n",
    "            # same geohash with precision 3\n",
    "            if climate[\"geohash\"][:3] == hotspot[\"geohash\"][:3]:\n",
    "                # hotspots are categorized only if there are fire_events\n",
    "                if hotspot[\"geohash\"] in fire_events:\n",
    "                    if (\n",
    "                        climate[\"air_temperature_celcius\"] > 20\n",
    "                        and climate[\"GHI_w/m2\"] > 180\n",
    "                    ):\n",
    "                        hotspot[\"cause\"] = \"natural\"\n",
    "                    else:\n",
    "                        hotspot[\"cause\"] = \"other\"\n",
    "                    print(f\"{hotspot['cause']} fire detected!\")\n",
    "\n",
    "            # clean up hotspot data\n",
    "            hotspot[\"timestamp\"] = datetime.strptime(\n",
    "                hotspot[\"datetime\"], \"%Y-%m-%dT%H:%M:%S\"\n",
    "            ).strftime(\"%H:%M:%S\")\n",
    "            hotspot.pop(\"producer_id\")\n",
    "            hotspot.pop(\"geohash\")\n",
    "            hotspot.pop(\"datetime\")\n",
    "\n",
    "            climate[\"hotspots\"].append(hotspot)\n",
    "\n",
    "    # clean up climate data\n",
    "    climate.pop(\"producer_id\")\n",
    "    climate.pop(\"latitude\")\n",
    "    climate.pop(\"longitude\")\n",
    "    climate.pop(\"geohash\")\n",
    "\n",
    "    climate[\"date\"] = datetime.strptime(climate[\"date\"], \"%Y-%m-%d\")\n",
    "    return climate\n",
    "\n",
    "\n",
    "def process_batch(df, epoch_id):\n",
    "    climate = None\n",
    "    aqua = []\n",
    "    terra = []\n",
    "\n",
    "    # iterate through batch dataset and assign to appropriate list based on producer_id\n",
    "    dataset = df.collect()\n",
    "    for record in dataset:\n",
    "        data = json.loads(record.value)\n",
    "\n",
    "        # encode geohash based on latitude and longitude\n",
    "        data[\"geohash\"] = pgh.encode(data[\"latitude\"], data[\"longitude\"], precision=5)\n",
    "\n",
    "        if data[\"producer_id\"] == \"climate_producer\":\n",
    "            climate = data\n",
    "        elif data[\"producer_id\"] == \"aqua_producer\":\n",
    "            aqua.append(data)\n",
    "        elif data[\"producer_id\"] == \"terra_producer\":\n",
    "            terra.append(data)\n",
    "        else:\n",
    "            print(\"Invalid producer_id....Skipping\")\n",
    "\n",
    "    print(\n",
    "        f\"received {0 if not climate else 1} climate records, {len(aqua)} aqua records, {len(terra)} terra records\"\n",
    "    )\n",
    "\n",
    "    # only proceed to process data for current window if there's a climate data\n",
    "    # climate date form the basis of a document in the DB hence no point in processing if there is no data\n",
    "    if climate:\n",
    "        fire_events, hotspots = group_hotspots(aqua, terra)\n",
    "        measurement = find_correlation(climate, hotspots, fire_events)\n",
    "        submit_datapoint(measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure spark writer to process stream in mini batches\n",
    "writer = (\n",
    "    kafka_sdf.writeStream.format(\"Console\")\n",
    "    .option(\"checkpointLocation\", \"climate-hotspots-checkpoint\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=f\"{BATCH_INTERVAL} seconds\")  # trigger action in batches\n",
    "    .foreachBatch(process_batch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    query = writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by CTRL-C. Stopping query.\")\n",
    "finally:\n",
    "    query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
